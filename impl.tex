\section{Implementation} \label{sec:implementation}
There are a couple of interesting things to discuss with respect to our current implementation, even though they do not necessarily contribute to the core algorithm of this paper. In general, the actual C\# code is very similar to the pseudo-code used in this paper, but it contains more detail and uses language features that are not highly relevant in this context, albeit interesting in their own right (e.g. LINQ expressions, cooperative concurrency with async-await).


\paragraph{Detecting Changes.} The propagation algorithm described in \S~\ref{sec:cp} assumes that we can detect whenever the state of a grain changes. Even though that might be possible, we opted for a simpler approach and used a \textit{heuristic}: we assume that every method call that was not invoked in context of a reactive computation, might change the grain's state. By doing so we over-approximate the operations that might cause state changes. In the same vain that programmers have to indicate to the runtime when they want the state to be persisted (using a dedicated \texttt{WriteStateAsync()} method), we could in the future also provide a call that invalidates the grain's summaries. For persisted grains, we could even re-use the persistence call, since the programmer already clearly indicates significant state changes have been made.

\paragraph{Garbage Collection.} Even though the result of the reactive computation might not be required any more, the programmer might still be holding on to a reference of the reactive computation object, or even just a tracker. This would prevent the runtime from cleaning up the entire distributed dependency graph. In order to prevent this, we instruct the programmer to always create reactive computations and trackers within a C\# \texttt{using} block (as shown in Fig. \ref{fig:impl} Lines 1 and 3). This guarantees that the reactive computation is directly disposed when it's not used any more. Consequently, the bipartite graph will only exist, and thus produce overhead, as long as anyone is using it, giving the 90 second time frame for cleaning up.

% This Figure can easily be removed when the page limit is reached %%
\begin{figure}
\begin{lstlisting}
using(var rc = GrainFactory.CreateReactiveComputation(
									() => Grain[myuserid].GetTimeline())) {
	using(var resulttracker = rc.GetResultTracker()) {

		while (interested) {
			try {
				var result = await resulttracker.NextResult();
				display(result);
			} catch(TimeoutException) { 
				display("server is not responding, retrying...");
			} catch(DivisionByZeroException) {
				display("can't divide by zero");			
			}
		}
	}
}
\end{lstlisting}
\caption{Proposed solution, including the \texttt{using} constructs that make sure the graph is garbage collected when no longer required.}\label{fig:impl}
\end{figure}

\paragraph{Non-Determinism.}
Whenever a method is called from within the context of a reactive computation, it is now prone to being re-executed without the programmer explicitly performing the call. Furthermore, we assume that when such invocation is performed twice with the same grain state (and the grains it depends on), the result is the same. In other words, the algorithm disregards any other source of non-determinism such as clock queries and I/O. The programmer should thus avoid using these operations in a reactive computation. Keep in mind that this solution is geared towards performing some kind of \emph{query} over the distributed state, thus it makes sense to make this assumption. Currently, we do no statically or dynamically enforce this, because some type of non-deterministic operations might still be useful for example for debugging purposes. On the other hand, it might be interesting to explore extensions of the algorithm that do explicitly incorporate this by for example recording and re-using non-deterministic operations.

\paragraph{Exception Handling}
As can be seen from Fig.~\ref{fig:impl} (lines 11-13), whenever an exception occurs somewhere down the reactive computation, we don't simply stop the computation and deconstruct the dependency graph. Instead we assume the exception just occurred due to some current state of the grains, but following states might produce results again. The exception can be catched (1) either somewhere upstream in the reactive computation itself, or (2) on the \texttt{NextResult()} call of a tracker if it wasn't caught. This also provides a uniform way of handling both distributed exceptions (timeout) (Fig.~\ref{fig:impl} line 9) and computational exceptions (Fig.~\ref{fig:impl} line 13).


We have implemented reactive computations as extensions to Orleans, an open-source distributed actor framework for .NET available on GitHub ["37"]. The Orleans runtime already provides the needed distributed protocols for managing the creation, placement, discovery, recovery, and load-balancing of grains. What we added is (a) extensions to the grain objects to store summaries, (b) interception points for grain calls, (b) modifications to the grain scheduler to distinguish between reactive and normal execution mode, and (c) a silo-wide cache manager. 

In an object-oriented imperative language like C\#, we cannot statically determine whether a grain operation modifies the grain state. Thus, we conservatively trigger change propagation after \emph{any} operation on a grain. This is not as expensive as it may seem at first, because if the grain state has not changed, re-execution of the summary produces the same result, and propagation stops. Still, the programmer can annotate an operation as read-only to avoid this overhead; also, we assume that any operation called as part of a reactive computation does not change the grain state, and avoid summary re-execution in that case.

\hidden
{
\subsection{Runtime Implementation}

Under the hood, the runtime must provide the needed distributed protocols for managing the creation, placement, discovery, recovery, and load-balancing of actors. By design, the application layer is largely unaware of how these details. Nevertheless, we briefly describe the mechanisms used by the Orleans system here. 

\mypar{Grain Directory} Grains can be active (there exists an instance of it on some machine) or inactive (otherwise). The runtime maintains a distributed directory (based on the consistent hashing algorithm)  for tracking active grains. When an inactive grain is accessed, the runtime automatically activates it, and places it on a randomly selected server. If a grain is not accessed for a prolonged (configurable) time, it is deactivated and removed from the grain directory. 

\mypar{Silo Failures} Under the hood, the runtime tracks all participating servers, called \emph{silos}, using a membership protocol. The set of members can change when administrators choose to increase or decrease the number of servers, or when servers fail, which is detected automatically.  For \emph{volatile} grains, the grain state is lost on failure. For \emph{persistent} grains, the grain state is saved to persistent storage after each change, and loaded from persistent storage when activated. 
 

--- The actual C\# code is similar, but contains more detail and uses language features that are not highly relevant in this context, albeit interesting in their own right (e.g. LINQ expressions, cooperative concurrency with async-await).

--- ResultTrackers have some interesting properties that make them well suited for situations where updating the display requires I/O, such as when communicating with a remotely connected client device:   (1) result trackers may skip intermediate versions: only the latest result matters, and (2) result trackers return a task that can be efficiently awaited without blocking the thread (using C\# language support for async/await \cite{Bierman2012}).  Also, it is possible to use multiple result trackers for the same reactive computation, and each one can consume results at its own speed.

--- reactive computations available on silo or client
}

