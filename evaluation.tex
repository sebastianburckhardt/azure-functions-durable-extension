\section{Evaluation}

When operating a service, the overall performance objective is to provide acceptable service latency at minimal cost. The cost can be lowered by running fewer servers - but this increases the latency because requests spend more time waiting for contended resources (such as network, storage, CPU, threads, locks, and so on). This tradeoff is an essential challenge for developers writing an elastic service application. 

To clarify the value that our mechanism provides to service architects, we now quantify both (a) its latency overhead and (b) its resource consumption, by comparing them to alternative solutions, including periodic polling (\S\ref{sec:polling}) and explicit change propagation at the application level (\S\ref{sec:observers}). To this end, we designed two series of experiments that measure low-load latency (\S\ref{sec:latency}), and variable-load throughput (\S\ref{sec:throughput}).

\paragraph{Application Model.}  
We model the application using  \emph{item} grains that are observed by \emph{view} grains. Each view depends on a fixed number of items, selected at random at the beginning of the test. Views are updated when items change, in a manner that depends on the chosen propagation solution, one of \emph{polling} (\S\ref{sec:polling}), \emph{explicit propagation} (\S\ref{sec:observers}), or \emph{automatic propagation} (\S\ref{sec:reactive}). Moreover, we vary the number of items and views to simulate different workloads (Table~\ref{tab:param}).  For example, a high \emph{fan-out} (= average number of views that depend on an item) means that whenever an item is mutated, many views need to be updated.  

 
The items and views run on five Orleans silos deployed as a Windows Azure cloud service. The robots run on 10 load generator servers. All processors have 8 cores, 14GB of RAM, and run at 1.6 GHz. To account for unexpected variations, we made sure to run each experiment series on at least 2 different datacenters, on at least 3 different days, and running the experiments in different order. Between runs, absolute numbers can vary up to 10\%, but the relative performance of the various solutions were stable. Note that we achieved this only after investing substantial work into our experimentation framework. 


\begin{figure}
\centering
\includegraphics[scale=.5, viewport=0 346 309 540]{figs/tp-setup} 
\caption{Experimental setup for throughput experiments.}\label{fig:tp-setup}
\end{figure}

\begin{table}
\begin{tabularx}{.99\columnwidth}{@{}Xrrrr@{}} \toprule
Name		& \#items 	& \#views & \#deps. & max robots\\ \midrule
low-load	&  600        	& 20       & 4 & n/a \\
fanout-1	& 20,000	& 20,000 & 1& 2,000 \\
fanout-20	& 10,000	& 20,000 & 10 & 2,000\\
fanout-200 & 1,000	& 20,000 & 10& 1,000\\  
\end{tabularx}
\caption{Parameter combinations we used.}\label{tab:param}
\end{table}


\subsection{Latency}\label{sec:latency}

Our latency experiments use the low-load parameters (Table~\ref{tab:param}) to eliminate delays caused by queueing and contention. We measure two types, called \emph{query latency} and \emph{propagation latency}. Latencies are measured 4000 times each (200 times per view, separated by 500ms). We describe the results using the median and lower and upper quartiles.\footnote{Average and standard deviation are unsuitable statistics because of the long tail of the distribution.}

\subsubsection{Query Latency}

\begin{figure}
\begin{lstlisting}
grain View
{
	state Deps: Item[numdeps]; 
	op Query(sequential: bool) : int[]
	{	
		var result = new int[numdeps];
		if (sequential) {
			for (0 <= i < numdeps)
				result[i] = Deps[i].GetValue();
		} else {
			parallel for (0 <= i < numdeps)
				result[i] = Deps[i].GetValue();
		}
		return result;
	}
	op OneTimeReactiveQuery(sequential: bool) : int[]
	{
	 	var rc = CreateReactiveComputation(
	 										() => Query(sequential));
		var result = await rc.GetResultTracker.NextResult();
		rc.Dispose();
		return result;
	}
}
\end{lstlisting}
\caption{Queries expressing how views depend on items.}\label{fig:queries}
\end{figure}

\begin{figure}
\centering
\begin{tabular}{@{}c@{}}
\includegraphics[width=\columnwidth, viewport=67 322 534 477]{figs/query-latencies}\\[.2in]
\includegraphics[width=\columnwidth, viewport=54 354 530 444]{figs/push-latencies}\\[.2in]
\includegraphics[width=\columnwidth, viewport=85 354 534 444]{figs/polling-latencies}\\
\end{tabular}
\caption{Measured latencies under low load, in milliseconds. The line in the middle of each box is the median, and the left and right edge are the first and third quartile. \textbf{(top)} Latencies for parallel and sequential queries; \textbf{(middle)} latencies for mutation response, application-level propagation using direct messages, and automatic propagation using result trackers; \textbf{(bottom)} propagation latencies for the polling solution at various frequencies.}\label{fig:querylatencies}
\end{figure}

For measuring query latency, we execute a query on a view that calls each of four items (either sequentially or in parallel) to collect some value and returns the values in an array (Fig.~\ref{fig:queries}). We now compare the latency when executed normally (\lstinline|Query|) to the latency when executed as a reactive computation (\lstinline|OneTimeReactiveQuery|). The normal latency for the sequential and parallel versions are shown in the first two rows of Fig.~\ref{fig:querylatencies} (top). The median latency is about 1.2ms for the parallel query and 3.8ms for the sequential query. This is consistent with the round-trip time of a typical grain call taking a bit less than 1ms. For the reactive queries, we distinguish two cases. If  the relevant summaries are not already cached on the silo, the query takes about 25\% longer than normal (rows 2,3). This overhead is caused by the installation and removal of the summary caches, and by scheduling overhead of our reactive caching implementation. However, if summaries for the items are already cached on the silo (for example, if another view is tracking the same items), the latency of \lstinline|OneTimeReactiveQuery| is less than 200$\mu$s (rows 4,5) because remote calls can be completely avoided. 

The results demonstrate that (1) the latency overhead of constructing the dependency graph is modest, and (2)  the caching effect alone can improve latency, even if not using the reactive features.

\subsubsection{Propagation Latency}

For measuring propagation latency, a view sends an update message to an item it depends on, and measures how much time elapses (a) until it receives the mutation response, and (b) until it receives the change propagation. 

First, we measured the speed of explicit propagation, where each item sends a notification message to all dependent views when mutated. This establishes a lower bound on propagation speed in the Orleans framework. The results show that the propagation message arrives right after the mutation completes: close to 1ms after calling the mutation operation (rows 2,3 of Fig.~\ref{fig:querylatencies} (middle)). This is consistent with the underlying system sending the mutation response message and the propagation message at about the same time. 

Second, we measured the speed of automatic propagation provided by our reactive caching algorithm. In that case, the propagation takes about 300$\mu$s longer (row 3), due to the scheduling overhead of our implementation.

Finally, we looked at the propagation speed of a polling-based solution. Fig.~\ref{fig:querylatencies} (bottom) shows the measured latencies, using a sequential query and various polling intervals. As expected, we see a median propagation time in the neighborhood of half of the polling interval plus the query latency, and a wide inter-quartile distance. Note that polling as frequently as every 100ms is not advisable in practice (we show this in the throughput section). Reasonable polling intervals are more typically between 3 and 30 seconds, meaning that the median propagation speed is 1.5--15 seconds which is three orders of magnitude slower than for explicit or automatic change propagation.  

The results show that our automatic propagation performs much better than a polling solution, while having the same simple programming model; and is not much slower than explicit change propagation.

\subsection{Throughput}\label{sec:throughput}

For the throughput experiments, we generate external load as shown in Fig.~\ref{fig:tp-setup}. The load generator contains up to 2000 robots; each runs a continuous loop that sends requests to the service, either to read a view, or to update an item. The percentage of updates in the mix is configurable. 

Each experiment gradually increases the robots and measures the throughput over time. The result is a curve that shows how throughput (number of requests handled per second) responds to load (number of requests concurrently in flight). For example, for the \lstinline|fanout20| configuration and a request mix containing 10\% updates, we obtained the curves shown in Fig.~\ref{fig:tp-curves}; each line corresponds to one experiment, and each bundle of lines corresponds to several experiments using the same propagation mechanism. 

The best possible throughput is achieved when change propagation is turned off entirely - because all resources are used for handling requests. Here, we reach close to 120k requests per second. But for a given propagation mechanism, the achieved throughput is lower, commensurate with the resources diverted to that mechanism: explicit propagation (dotted lines) reaches about 45k, automatic  propagation reaches about 35k. For 10s-polling, the throughput reaches about 70k (better than automatic or manual propagation), but for 1s-polling, it reaches only about 20k (worse than automatic or manual propagation). 

To compare the solutions across different configurations and update ratios, we ran similar ramp-up experiments for all combinations, but extend them by running at peak load for a while to measure an average  peak throughput. The results are shown in Fig.~\ref{}. 

  leads to a commensurate drop in throughput.  throughput decreseas
\begin{itemize}
\item Disabling all change propagation gives us the maximal 
\end{itemize}

  in Figof requests that complete within a sliding window. . we start with zero robots and add 
To get insight into how the service responds to load, we increase the load gradually by adding robots over time we do not start at maximum load, but gradually increase the load 


\begin{figure}
\noindent
\centering
\includegraphics[width=\columnwidth, viewport=164 80 633 531]{figs/tp-curves} 
\caption{Throughput response of different propagation mechanisms, for fanout-20 with 10\% updates.}\label{fig:tp-curves}
\end{figure}

\begin{figure*}
\noindent
\centering
\includegraphics[width=\textwidth, viewport=58 201 726 550]{figs/tp-all} 
\caption{Peak throughput for all configurations.}\label{fig:tp-all}
\end{figure*}

