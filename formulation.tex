\section{Virtual Actor Model}\label{sec:virtualactors}

Actors have emerged as a useful abstraction for the middle tier of scalable service applications that run on virtualized cloud infrastructure in a datacenter . In such systems, each actor is an object with a user-defined meaning, identity, state, and operations. For example, actors can represent user profiles, articles, game sessions, devices, bank accounts, or chat rooms. Actors resemble miniature servers: they do not share memory, but communicate asynchronously, and can fail and recover independently. One of the main benefit of actor systems is that they \emph{scale horizontally}, because the actor instances can be distributed across a cluster of servers.

In a traditional bare-bones actor system, the developer remains responsible for the creation, placement, discovery, recovery, and load-balancing of actors. A newer generation of actor models \cite{orleans,orbit,sfactors}, called virtual actor models, automate all of these aspects. The developer specifies only (1) a unique key for identifying each actor, and (2) how to save and load the actor state to/from external storage, if persistence is desired. As virtual actor systems can activate and deactivate actors based on use, they strongly resemble distributed caches \cite{memcached} and provide similar performance benefits. 

Following the terminology \cite{orleanstr,orleans-socc}, we call the virtual actors \emph{grains}. For each grain type, the code defines a (1) key that identifies instances, (2)  fields that define the grain state, and (3) operations supported by the grain. The operations of a grain can read and modify the grain state, and can call operations on other grains, but cannot directly read or write the state of other grains (encapsulation). 
 
\subsection{Example: Chirper}

As a running example, we use a service called \emph{Chirper} (based on the Orleans sample with the same name). It models a typical social application where users post messages to their own timeline, and view a timeline containing all messages posted by people they are following.
The code for the chirper service is shown in Fig.~\ref{fig:chirper}. We use imperative pseudocode and an imaginary actor DSL (domain-specific language) to focus on the programming model and minimize distractions (such as framework details or C\# language features). 

\begin{figure}
\begin{lstlisting}
grain User[userid: string] %\label{l:def}%
{
	// grain state
	state Messages: map<<time,string>>;%\label{l:msgs}%
	state Follows: set<<string>>;%\label{l:flws}%
	// update operations
	op Post(t: time, msg: string) { Messages[t] = msg; }%\label{b:uo}%
	op Unpost(t: time) { Messages.remove(t); }
	op Follow(id: string) { Follows.add(id); }
	op Unfollow(id: string) { Follows.remove(id); }%\label{e:uo}%
	// query operations
	op GetMessages(from: time) : list<<pair<<time,string>>>> { %\label{l:getm}%
		var msgs = new list<<pair<<time,string>>>>();
		foreach(var m in Messages)
			if (m.key >= from)
				msgs.add((m.key, m.value));
		return msgs;
	}
	op GetTimeline(from: time): list<<pair<<time,string>>>> {%\label{l:gettl}%
		var msgs = GetMessages(from);%\label{l:loc}%
		parallel foreach(var f in Follows) {%\label{l:par}%
			// retrieve messages from user f by remote call
			var fm = User[f].GetMessages(from); %\label{l:rcall}%
			foreach(var m in fm) { msgs.add(m); }%\label{l:add}%
		}
		msgs.sort();%\label{l:sort}%
		return msgs;%\label{l:ret}%
	}
}
\end{lstlisting}
\caption{Pseudocode for the Chirper service example.}\label{fig:chirper}
\end{figure}

There is only one type of grain, called \lstinline|User|.  A user grain encapsulates data associated with a user, and is identified by a string called \lstinline{userid} (\lref{def}). Each user grain stores the messages posted by that user in a map data structure \lstinline|Messages| (\lref{msgs}), keyed by timestamp. The current list of followed users is stored in \lstinline|Follows| (\lref{flws}). The grain state can change in response to update operations \lstinline|Post|, \lstinline|Unpost|, \lstinline|Follow|, \lstinline|Unfollow| (\rref{uo}) , which are invoked by the chirper client in response to clicking respective buttons.

The operation \lstinline|GetMessages(from)| (\lref{getm}) returns all messages by this user since the time indicated by \lstinline|from|; it straightforwardly computes the result by iterating over \lstinline{Messages}.  

The operation  \lstinline|GetTimeline(from)|  (\lref{gettl}) is more interesting. It returns a sorted list of all messages posted by the user \emph{and all followed users} since  the time indicated by \lstinline|from|. It computes the result as follows: first, it retrieves the locally stored messages (\lref{loc}). Then, \emph{in parallel} (\lref{par}), it calls \lstinline|GetMessages| on all followed users. These calls target other grains, by constructing a grain reference \lstinline|User[f]| to the grain for userid \lstinline|f| (\lref{rcall}). The returned messages are all added to the list \lstinline|msgs| (\lref{add}). After all the parallel tasks complete, the messages are sorted (\lref{sort}) and returned (\lref{ret}).

\subsection{Failure Model}

A grain may fail at any time, even when in the middle of executing an operation, and this can be observed by the application. The runtime does provide some support for simplifying the failure handling at the application level. 

\mypar{Recovery} If a grain is hosted on a server and that server fails, the runtime activates a fresh instance of the grain the next time it is accessed. 

\mypar{Persistence} Optionally, programmers can declare grains to be \emph{persistent}, which means their state is saved in external persistent storage. The state of a persistent grain is saved after each operation that updates the state, and loaded from storage when a grain is activated. The \lstinline|User| grains in the chirper example are persistent. 

\mypar{Timeouts} All grain operations throw a special timeout-exception if no response is received after a configurable time limit (the default is 30 seconds). This helps to avoid hangs and deadlocks at the application level if a grain fails before responding, or becomes unavailable for any reason.

If an operation times out, the caller does not know whether its effect was performed or not. To handle this issue, applications often design update operations to be \emph{idempotent} \cite{kapil}, so they can be safely retried after encountering a timeout-exception. For example, all the operations of the \lstinline|User| grain in Fig.~\ref{fig:chirper} are idempotent.

\section{Reactive Computations}\label{sec:formulation}

Suppose now that we want our chirper client to not just display a frozen snapshot of \lstinline|GetTimeline|, but to react to changes in the application state and refresh the user display accordingly. In the absence of any special support for reactive programming, our options are to use polling (\S\ref{sec:polling}) or to implement change propagation explicitly at the application level (\S\ref{sec:observers}). Next, we briefly discuss those two solutions, and then show the solution based on runtime support for reactive computations (\S\ref{sec:reactive}). 

\begin{figure}
\begin{lstlisting}
while (interested) {
	try {
		var result = Grain[myuserid].GetTimeline();
		display(result);
	} catch(TimeoutException) { 
		display("server is not responding, retrying...");
	}  
	await delay(5000); // wait for 5 seconds between refresh
}
\end{lstlisting}
\caption{Possible solution using client-side polling.}\label{fig:polling}
\end{figure}

\subsection{Client-side Polling}\label{sec:polling}

 A straightforward solution is to call \lstinline|GetTimeline| periodically on the client to retrieve the latest state (Fig.~\ref{fig:polling}). This solution is easy to understand, requires no changes to the service code, and gracefully handles transient exceptions (such as caused by high load or server failures). However, it confronts us with a unpleasant tradeoff when choosing the \emph{polling frequency}: infrequent polling means the displayed result can lag significantly behind the latest result; but frequent polling dramatically increases the load on our service. We demonstrate this tradeoff experimentally in section \ref{sec:evaluation} .

\begin{figure}
\begin{lstlisting}
// new additions to  grain state
state Observers: map<<Observer,time>>; 
state Followers: set<<userid>>; 	

// new update operations
op AddFollower (userid: string)
	{ Followers.Add(userid); }
op RemoveFollower (userid: string)
	{ Followers.Remove(userid); }
op AddObserver (client: Observer, from: time)
   : list<<pair<<time,string>>>>{%\label{l:cs}%
 	Observers[client] = time; 
 	return GetTimeline(from);
}
op RemoveObserver (client: Observer)
	{ Observers.Remove(client); }

// modifications to existing update operations
op Post(tstamp: time, msg: string) { 
	...
	NotifyFollowers();  NotifyObservers();
} 
op Unpost(tstamp: time) { 
	...
	NotifyFollowers();  NotifyObservers();
}
op Follow(userid: string) {  
	... 
	User[userid].AddFollower(this.userid);
	NotifyObservers();
}
op Unfollow(userid: string) { 
	...  
	User[userid].RemoveFollower(this.userid);
	NotifyObservers(); 
}

// new propagation operations
op NotifyObservers() {
	foreach(var (client,from) in Observers) {
		var latest = GetTimeline(from);
		client.Notify(latest);
	}
}
op NotifyFollowers() {
	parallel foreach(var f in Followers) { f.NotifyObservers(); }
}
\end{lstlisting}
\caption{Cautionary sketch of what it may take to apply the observer pattern to the \lstinline|User| grain from Fig.~\ref{fig:chirper}.}\label{fig:observers}
\end{figure}

\subsection{Explicit Propagation}\label{sec:observers}

A more challenging, but possibly better-performing solution is to add code to our service that explicitly tracks dependencies and pushes changes to the client. Fig.~\ref{fig:observers} shows a sketch of how we can apply the observer design pattern to the User grain (many other variations are possible). Clients can register themselves as observers (\lref{cs}), which means they receive notifications containing the latest result. Each grain  tracks observing clients and followers and notifies them. \emph{But this solution leaves much to be desired. } We have polluted the grain state with information about observers, and it is unclear whether this state should be persisted (and thereby pollute our storage as well) or not (and thereby cause issues when grains fail). Also, as written, changes are still not propagated very efficiently --- there remain many potential optimizations, and opportunities for introducing subtle bugs. 

\hidden
{\begin{itemize}
\item It is not clear if the client observers should be considered part of the normal grain state and persisted along with it. If observer lists are not persisted, they are lost on failures - but if they are persisted, we pay a performance price.
\item We are now saving redundant information in the user grains (\lstinline{Follows} and \lstinline{Followers} represent the same relation), and can observe inconsistencies  (the state of multiple grains is not saved atomically).  
\item Changes are not propagated selectively: a change notification forces the client to reexecute the entire query, even if only a small part of the computation is different.
\end{itemize}
}

\hidden{
Conceivably, a skilled programmer can solve all of these issues at the application level --- but in our experience, this is a road that leads to complex, error-prone code that is susceptible to deadlocks and subtle race conditions. There is a better way.
}


\begin{figure}
\begin{lstlisting}
var rc = CreateReactiveComputation(%\label{l:crc}%
									() => Grain[myuserid].GetTimeline()   	);
var resulttracker = rc.GetResultTracker();%\label{l:crt}%

while (interested) {%\label{b:pl}%
	try {
		var result = await resulttracker.NextResult();%\label{l:await}%
		display(result);
	} catch(TimeoutException) { 
		display("server is not responding, retrying...");
	}
}%\label{e:pl}%
\end{lstlisting}
\caption{Proposed solution: runtime support for reactive computations.}\label{fig:rcapi}
\end{figure}


\subsection{Automatic Propagation}\label{sec:reactive}

By adding support for reactive computations into the virtual actor runtime, we can spare the programmer considerable hassle, and let them benefit from an automatic, optimized distributed algorithm for dependency tracking and change propagation. 

We propose a novel API for reactive computations over virtual actors (Fig.~\ref{fig:rcapi}). It is purposefully similar to the client-side polling solution (Fig.~\ref{fig:polling}) and likewise does \emph{not require any changes to the service code}. 

First, the programmer creates a \emph{reactive computation} object (\lref{crc}), passing an anonymous function that describes the computation. This instructs the runtime to construct a dependency graph which continuously tracks the result of that computation and propagates changes via pushing, until the rc object is disposed.

To consume the results, the programmer simply creates a \emph{result tracker} object (\lref{crt}). The result tracker operates like an enumerator that produces both the initial result, and any successive changed results, when we call \lstinline|NextResult| (\lref{await}). %\footnote{to avoid potential confusion: note that the result tracker is \emph{not} an enumerator for the produced timeline; rather, each call to \lstinline|NextResult| returns the latest version of the \emph{entire} timeline.}
Note that the code in the loop (\rref{pl}) looks just like the client-side polling solution (Fig.~\ref{fig:polling}), and has the same failure and exception semantics, but we no longer need to specify an arbitrary polling delay --- instead, the loop simply waits for a new result to arrive, and then delivers it immediately. With compiler support for \lstinline|await|, as in C\# \cite{Bierman2012}, this is just as efficient as using callbacks, because the compiler constructs continuations that do not block the thread.

Exactly how the client processes the results is outside the scope of this work, and likely depends on the chosen client framework. For chirper  we used react \cite{react}; it lets us simply assign the result to the model, and automatically does the diffing.

%adding support for reactive computations into the virtual actor runtime, we can avoid the hassle of changing the application code, and  retain the elegant simplicity and well-defined failure handling of the client-side polling solution (Fig.\ref{}). Our proposed API is shown

%The reason is that the virtual actor model already provides us with a clean decomposition of computations and clear interception points (grain calls) that can be cached (memoized) and updated reactively. 
 
